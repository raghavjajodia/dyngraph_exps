{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import register_data_args, load_data\n",
    "from dgl.data import BitcoinOTC\n",
    "import datetime\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import time\n",
    "\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dim = 256\n",
    "n_layers = 2\n",
    "dropout = 0.2\n",
    "learning_rate = 0.01\n",
    "wt_decay = 5e-4\n",
    "stpsize = 15\n",
    "n_epochs = 10\n",
    "out_path = '../btc_static/'\n",
    "self_loop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSelfEdges(edgeList, colFrom, colTo):\n",
    "    mask = edgeList[:, colFrom] - edgeList[:, colTo] != 0\n",
    "    edgeList = edgeList[mask]\n",
    "    return edgeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading \n",
    "graphs = []\n",
    "\n",
    "data  = np.loadtxt('../soc-sign-bitcoinotc.csv', delimiter=',').astype(np.int64)\n",
    "data[:, 0:2] = data[:, 0:2] - data[:, 0:2].min()\n",
    "data = removeSelfEdges(data, 0, 1)\n",
    "num_nodes = data[:, 0:2].max() - data[:, 0:2].min() + 1\n",
    "delta = datetime.timedelta(days=14).total_seconds()\n",
    "time_index = np.around(\n",
    "    (data[:, 3] - data[:, 3].min())/delta).astype(np.int64)\n",
    "\n",
    "prevind = 0\n",
    "for i in range(time_index.max()):\n",
    "    g = DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    row_mask = time_index <= i\n",
    "    edges = data[row_mask][:, 0:2]\n",
    "    rate = data[row_mask][:, 2]\n",
    "    diffmask = np.arange(len(edges)) >= prevind\n",
    "    g.add_edges(edges[:, 0], edges[:, 1])\n",
    "    g.edata['feat'] = torch.FloatTensor(rate.reshape(-1, 1))\n",
    "    g.edata['diff'] = diffmask\n",
    "    g.ndata['feat'] = torch.zeros(num_nodes, node_dim)\n",
    "    \n",
    "    if self_loop == True:\n",
    "        g.add_edges(g.nodes(), g.nodes())\n",
    "        \n",
    "    selfedgemask = np.zeros(g.number_of_edges(), dtype = bool)\n",
    "    selfedgemask[-g.number_of_nodes():] = True\n",
    "    g.edata['self_edge'] = selfedgemask\n",
    "    \n",
    "    graphs.append(g)\n",
    "    prevind = len(edges)\n",
    "    \n",
    "train_graph = graphs[94]\n",
    "valid_graphs = graphs[95:109]\n",
    "test_graphs = graphs[109:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        self.distrlayer = nn.Linear(2 * n_hidden, 64, bias = True)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.outlayer = nn.Linear(64, 1, bias = True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features, g):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h)\n",
    "        srcfeatures = torch.stack(list(map(lambda nd: h[nd], g.all_edges()[0])))\n",
    "        destfeatures = torch.stack(list(map(lambda nd: h[nd], g.all_edges()[1])))\n",
    "        edgefeatures = torch.cat((srcfeatures, destfeatures), dim = 1)\n",
    "        outputs = self.nonlinear(self.distrlayer(edgefeatures))\n",
    "        outputs = self.outlayer(outputs)\n",
    "        outputs = 20*torch.sigmoid(outputs) - 10\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, criterion, device, valid_graphs):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    #validation phase\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for val_graph in valid_graphs:\n",
    "            outputs = model(val_graph.ndata['feat'], val_graph)\n",
    "            labels = val_graph.edata['feat']\n",
    "            outputs = outputs[val_graph.edata['diff']]\n",
    "            labels = labels[val_graph.edata['diff']]\n",
    "            num_samples += labels.shape[0] \n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += (loss.item()* labels.shape[0])\n",
    "    \n",
    "    return epoch_loss/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_f1(model, criterion, device, valid_graphs):\n",
    "    model.eval()\n",
    "    \n",
    "    #validation phase\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, val_graph in enumerate(valid_graphs):\n",
    "            outputs = model(val_graph.ndata['feat'], val_graph)\n",
    "            labels = val_graph.edata['feat']\n",
    "            outputs = outputs[val_graph.edata['diff']]\n",
    "            outputs = outputs.round().long()\n",
    "            labels = labels[val_graph.edata['diff']]\n",
    "            \n",
    "            if i == 0:\n",
    "                all_outputs = outputs\n",
    "                all_labels = labels\n",
    "            else:\n",
    "                all_outputs = torch.stack((all_outputs, outputs))\n",
    "                all_labels = torch.stack((all_labels, labels))\n",
    "        all_outputs = all_outputs.detach().numpy()\n",
    "        all_labels = all_labels.detach().numpy()\n",
    "        f1score = f1_score(all_labels, all_outputs, average='micro')\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for supervised training\n",
    "def train_model(model, criterion, optimizer, scheduler, device, checkpoint_path, hyperparams, num_epochs=25):\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\"train\"] = {}\n",
    "    metrics_dict[\"valid\"] = {}\n",
    "    metrics_dict[\"train\"][\"loss\"] = {}\n",
    "    metrics_dict[\"train\"][\"loss\"][\"epochwise\"] = []\n",
    "    metrics_dict[\"valid\"][\"loss\"] = {}\n",
    "    metrics_dict[\"valid\"][\"loss\"][\"epochwise\"] = []\n",
    "        \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 9999999999999999\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{} \\n'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        #train phase\n",
    "        scheduler.step()\n",
    "        model.train() \n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        forward_start_time  = time.time()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(train_graph.ndata['feat'], train_graph)\n",
    "            labels = train_graph.edata['feat']\n",
    "            outputs = outputs[~train_graph.edata['self_edge']]\n",
    "            labels = labels[~train_graph.edata['self_edge']]\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss = loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        forward_time = time.time() - forward_start_time\n",
    "        \n",
    "        print('Train Loss: {:.4f} \\n'.format(epoch_loss))\n",
    "        metrics_dict[\"train\"][\"loss\"][\"epochwise\"].append(epoch_loss)\n",
    "        writer.add_scalar('training loss',\n",
    "                            epoch_loss)\n",
    "        \n",
    "        #validation phase\n",
    "        val_epoch_loss = evaluate_loss(model, criterion, device, valid_graphs)\n",
    "        print('Validation Loss: {:.4f} \\n'.format(val_epoch_loss))\n",
    "        metrics_dict[\"valid\"][\"loss\"][\"epochwise\"].append(val_epoch_loss)\n",
    "        \n",
    "        # deep copy the model\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'full_metrics': metrics_dict,\n",
    "        'hyperparams': hyperparams\n",
    "        }, '%s/net_epoch_%d.pth' % (checkpoint_path, epoch))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s \\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f} \\n'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9 \n",
      "\n",
      "----------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghav/anaconda3/envs/dgl_env/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    " # create GCN model\n",
    "model = GCN(node_dim, node_dim, n_layers, F.relu, dropout)\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(model_parameters, lr=learning_rate, weight_decay = wt_decay)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=stpsize, gamma=0.1)\n",
    "hyper_params = {'node_dim' : node_dim,\n",
    "    'n_layers' : n_layers,\n",
    "    'dropout' : dropout,\n",
    "    'wt_decay' : wt_decay }\n",
    "\n",
    "bst_model = train_model(model, criterion, optimizer, exp_lr_scheduler, device, out_path, hyper_params, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
